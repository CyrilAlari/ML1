{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gini\n",
    "\n",
    "from util_data import DataSet\n",
    "\n",
    "# added\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostRegressor, BaggingRegressor, GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.feature_selection import RFECV\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# preprocessing\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "data = DataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I think we can use panda as an alternative for analysis, since it helps us gain more insights into the data\n",
    "\n",
    "train = data.get_training_set()\n",
    "test = data.get_testing_set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a few models with standard parameters to find the best regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ListOfFunction = [LogisticRegression,AdaBoostRegressor,BaggingRegressor,GradientBoostingRegressor,RandomForestRegressor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# separate data and label\n",
    "X = train.drop(['id','target'],axis=1) # drop id and target from X, since \"id\" wouldn't do much help for prediction\n",
    "Y = train['target'].as_matrix()\n",
    "X =X.fillna(X.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take a training set of size 200 000 (more than 30% of the set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "Gini: -0.106, Max. Gini: 0.482, Normalized Gini: -0.221\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-e713d67c593f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mY_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mgini\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgini_visualization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/cyril2/Documents/3A/Machine Learning I/Project/ML1/proj1/gini.py\u001b[0m in \u001b[0;36mgini_visualization\u001b[0;34m(actual, pred, graph)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# Sort the actual values by the predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0msorted_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0msorted_actual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# print('Sorted Actual Values', sorted_actual)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "for func in ListOfFunction:\n",
    "    rfc = func()\n",
    "    rfc.fit(X[:200000],Y[:200000])\n",
    "    Y_pred = rfc.predict_proba(X[-50000:])\n",
    "    print(func.__name__)\n",
    "    gini.gini_visualization(Y[-50000:],Y_pred,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly Gradient Boosting seems best by now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to remove some unuseful features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# When there is calc we remove (see first_examples notebook)\n",
    "customX = X.drop(X.filter(like='calc').columns,axis=1)\n",
    "\n",
    "rfc = GradientBoostingRegressor()\n",
    "rfc.fit(X[:200000],Y[:200000])\n",
    "Y_pred = rfc.predict(X[-50000:])\n",
    "print(func.__name__)\n",
    "gini.gini_visualization(Y[-50000:],Y_pred,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make change in the number of estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# When there is calc we remove (see first_examples notebook)\n",
    "customX = X.drop(X.filter(like='calc').columns,axis=1)\n",
    "\n",
    "numbers = [10,20,40,60,80,100,150,200,300,400,500]\n",
    "results = []\n",
    "\n",
    "for i in numbers:\n",
    "    rfc = GradientBoostingRegressor(n_estimators = i)\n",
    "    rfc.fit(X[:200000],Y[:200000])\n",
    "    Y_pred = rfc.predict(X[-50000:])\n",
    "    print(func.__name__ + \" for \"+str(i)+\" classifiers\")\n",
    "    results.append(gini.gini_visualization(Y[-50000:],Y_pred,False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Summary\")\n",
    "plt.plot(numbers,results)\n",
    "plt.xlabel('Number of classifiers')\n",
    "plt.ylabel('Gini')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More precised tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try with unuseful features and normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Case 1 : Feature removal + Normalization\n",
    "customX = X.drop(X.filter(like='calc').columns,axis=1)\n",
    "customX = normalize(customX)\n",
    "\n",
    "numbers = [60,80,100,150]\n",
    "results = []\n",
    "\n",
    "for i in numbers:\n",
    "    rfc = GradientBoostingRegressor(n_estimators = i)\n",
    "    rfc.fit(customX[:200000],Y[:200000])\n",
    "    Y_pred = rfc.predict(customX[-50000:])\n",
    "    print(func.__name__ + \" for \"+str(i)+\" classifiers\")\n",
    "    results.append(gini.gini_visualization(Y[-50000:],Y_pred,False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Case 2 : No feature removal + Normalization\n",
    "customX = normalize(X)\n",
    "\n",
    "numbers = [60,80,100,150] # these seems to be the best parameters\n",
    "results = []\n",
    "\n",
    "for i in numbers:\n",
    "    rfc = GradientBoostingRegressor(n_estimators = i)\n",
    "    rfc.fit(customX[:200000],Y[:200000])\n",
    "    Y_pred = rfc.predict(customX[-50000:])\n",
    "    print(func.__name__ + \" for \"+str(i)+\" classifiers\")\n",
    "    results.append(gini.gini_visualization(Y[-50000:],Y_pred,False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Case 3 : Feature removal + No normalization\n",
    "customX = X.drop(X.filter(like='calc').columns,axis=1)\n",
    "\n",
    "numbers = [60,80,100,150]\n",
    "results = []\n",
    "\n",
    "for i in numbers:\n",
    "    rfc = GradientBoostingRegressor(n_estimators = i)\n",
    "    rfc.fit(customX[:200000],Y[:200000])\n",
    "    Y_pred = rfc.predict(customX[-50000:])\n",
    "    print(func.__name__ + \" for \"+str(i)+\" classifiers\")\n",
    "    results.append(gini.gini_visualization(Y[-50000:],Y_pred,False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Case 4 : No feature removal, no normalization\n",
    "customX = X\n",
    "\n",
    "numbers = [60,80,100,150]\n",
    "results = []\n",
    "\n",
    "for i in numbers:\n",
    "    rfc = GradientBoostingRegressor(n_estimators = i)\n",
    "    rfc.fit(customX[:200000],Y[:200000])\n",
    "    Y_pred = rfc.predict(customX[-50000:])\n",
    "    print(func.__name__ + \" for \"+str(i)+\" classifiers\")\n",
    "    results.append(gini.gini_visualization(Y[-50000:],Y_pred,False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More features removal ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correlated_features = [\"ps_reg_03\",\"ps_car_13\"]\n",
    "lacunar_features = [\"ps_car_03_cat\",\"ps_car_05_cat\"]\n",
    "func = GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "customX = X.drop(X.filter(like='calc').columns,axis=1).drop(correlated_features,axis=1)\n",
    "\n",
    "numbers = [60,80,100,150,200]\n",
    "results = []\n",
    "\n",
    "for i in numbers:\n",
    "    rfc = GradientBoostingRegressor(n_estimators = i)\n",
    "    rfc.fit(customX[:200000],Y[:200000])\n",
    "    Y_pred = rfc.predict(customX[-50000:])\n",
    "    print(func.__name__ + \" for \"+str(i)+\" classifiers\")\n",
    "    results.append(gini.gini_visualization(Y[-50000:],Y_pred,False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "customX = X.drop(X.filter(like='calc').columns,axis=1).drop(lacunar_features,axis=1)\n",
    "\n",
    "numbers = [60,80,100,150,200]\n",
    "results = []\n",
    "\n",
    "for i in numbers:\n",
    "    rfc = GradientBoostingRegressor(n_estimators = i)\n",
    "    rfc.fit(customX[:200000],Y[:200000])\n",
    "    Y_pred = rfc.predict(customX[-50000:])\n",
    "    print(func.__name__ + \" for \"+str(i)+\" classifiers\")\n",
    "    results.append(gini.gini_visualization(Y[-50000:],Y_pred,False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "customX = X.drop(X.filter(like='calc').columns,axis=1).drop(correlated_features,axis=1).drop(lacunar_features,axis=1)\n",
    "\n",
    "numbers = [60,80,100,150,200]\n",
    "results = []\n",
    "\n",
    "for i in numbers:\n",
    "    rfc = GradientBoostingRegressor(n_estimators = i)\n",
    "    rfc.fit(customX[:200000],Y[:200000])\n",
    "    Y_pred = rfc.predict(customX[-50000:])\n",
    "    print(func.__name__ + \" for \"+str(i)+\" classifiers\")\n",
    "    results.append(gini.gini_visualization(Y[-50000:],Y_pred,False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More and More ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "customX = X.drop(X.filter(like='calc').columns,axis=1).drop(correlated_features,axis=1)\n",
    "disbalance = []\n",
    "\n",
    "for col in customX.filter(like='bin').columns:\n",
    "    if(train[[col,'id']].groupby([col], as_index=False).count().loc[1]['id']< 20000 or train[[col,'id']].groupby([col], as_index=False).count().loc[0]['id']< 20000):\n",
    "        print(col)\n",
    "        disbalance.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "customX = X.drop(X.filter(like='calc').columns,axis=1).drop(correlated_features,axis=1).drop(lacunar_features,axis=1).drop(disbalance,axis=1)\n",
    "\n",
    "numbers = [150]\n",
    "results = []\n",
    "func = GradientBoostingRegressor\n",
    "\n",
    "for i in numbers:\n",
    "    rfc = GradientBoostingRegressor(n_estimators = i)\n",
    "    rfc.fit(customX[:200000],Y[:200000])\n",
    "    Y_pred = rfc.predict(customX[-50000:])\n",
    "    print(func.__name__ + \" for \"+str(i)+\" classifiers\")\n",
    "    results.append(gini.gini_visualization(Y[-50000:],Y_pred,False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rk = [1, 1, 1, 1, 1, 4, 1, 1, 3, 1, 1, 1, 1, 6, 1, 1, 1, 2, 1, 1, 1, 1, 1,\n",
    "       5, 1, 1, 1, 1, 1]\n",
    "\n",
    "customX = X.drop(X.filter(like='calc').columns,axis=1).drop(correlated_features,axis=1).drop(lacunar_features,axis=1).drop(disbalance,axis=1)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingRegressor for 100 classifiers\n",
      "Gini: 0.122, Max. Gini: 0.482, Normalized Gini: 0.253\n",
      "GradientBoostingRegressor for 100 classifiers\n",
      "Gini: 0.122, Max. Gini: 0.482, Normalized Gini: 0.253\n",
      "GradientBoostingRegressor for 100 classifiers\n",
      "Gini: 0.122, Max. Gini: 0.482, Normalized Gini: 0.253\n",
      "GradientBoostingRegressor for 130 classifiers\n",
      "Gini: 0.124, Max. Gini: 0.482, Normalized Gini: 0.257\n",
      "GradientBoostingRegressor for 130 classifiers\n",
      "Gini: 0.124, Max. Gini: 0.482, Normalized Gini: 0.257\n",
      "GradientBoostingRegressor for 130 classifiers\n",
      "Gini: 0.124, Max. Gini: 0.482, Normalized Gini: 0.257\n",
      "GradientBoostingRegressor for 150 classifiers\n",
      "Gini: 0.124, Max. Gini: 0.482, Normalized Gini: 0.258\n",
      "GradientBoostingRegressor for 150 classifiers\n",
      "Gini: 0.123, Max. Gini: 0.482, Normalized Gini: 0.255\n",
      "GradientBoostingRegressor for 150 classifiers\n",
      "Gini: 0.125, Max. Gini: 0.482, Normalized Gini: 0.259\n",
      "GradientBoostingRegressor for 170 classifiers\n",
      "Gini: 0.124, Max. Gini: 0.482, Normalized Gini: 0.257\n",
      "GradientBoostingRegressor for 170 classifiers\n",
      "Gini: 0.124, Max. Gini: 0.482, Normalized Gini: 0.257\n",
      "GradientBoostingRegressor for 170 classifiers\n",
      "Gini: 0.124, Max. Gini: 0.482, Normalized Gini: 0.258\n",
      "GradientBoostingRegressor for 190 classifiers\n",
      "Gini: 0.122, Max. Gini: 0.482, Normalized Gini: 0.253\n",
      "GradientBoostingRegressor for 190 classifiers\n",
      "Gini: 0.123, Max. Gini: 0.482, Normalized Gini: 0.255\n",
      "GradientBoostingRegressor for 190 classifiers\n",
      "Gini: 0.124, Max. Gini: 0.482, Normalized Gini: 0.257\n",
      "GradientBoostingRegressor for 230 classifiers\n",
      "Gini: 0.123, Max. Gini: 0.482, Normalized Gini: 0.255\n",
      "GradientBoostingRegressor for 230 classifiers\n",
      "Gini: 0.122, Max. Gini: 0.482, Normalized Gini: 0.252\n",
      "GradientBoostingRegressor for 230 classifiers\n",
      "Gini: 0.123, Max. Gini: 0.482, Normalized Gini: 0.255\n"
     ]
    }
   ],
   "source": [
    "\n",
    "numbers = [1,3,6]\n",
    "numbers_c = [100,130,150,170,190,230]\n",
    "results = []\n",
    "func = GradientBoostingRegressor\n",
    "\n",
    "for j in numbers_c:\n",
    "    selector = RFECV(rfc,step = 1)\n",
    "    selector.fit(customX[:200000], Y[:200000])\n",
    "    rk = selector.ranking_\n",
    "    for i in numbers:\n",
    "        highrk = []\n",
    "        for col,r in zip(customX.columns,rk):\n",
    "            if r>i:\n",
    "                highrk.append(col)\n",
    "        custom_X = X.drop(X.filter(like='calc').columns,axis=1).drop(correlated_features,axis=1).drop(lacunar_features,axis=1).drop(disbalance,axis=1).drop(highrk,axis=1)\n",
    "        rfc = GradientBoostingRegressor(n_estimators = j)\n",
    "        rfc.fit(custom_X[:200000],Y[:200000])\n",
    "        Y_pred = rfc.predict(custom_X[-50000:])\n",
    "        print(func.__name__ + \" for \"+str(j)+\" classifiers\")\n",
    "        results.append(gini.gini_visualization(Y[-50000:],Y_pred,False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
