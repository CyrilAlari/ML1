{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gini\n",
    "\n",
    "from util_data import DataSet\n",
    "\n",
    "# added\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostRegressor, BaggingRegressor, GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.feature_selection import RFECV\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# preprocessing\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "data = DataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I think we can use panda as an alternative for analysis, since it helps us gain more insights into the data\n",
    "\n",
    "train = data.get_training_set()\n",
    "test = data.get_testing_set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a few models with standard parameters to find the best regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ListOfFunction = [LogisticRegression,AdaBoostRegressor,BaggingRegressor,GradientBoostingRegressor,RandomForestRegressor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# separate data and label\n",
    "X = train.drop(['id','target'],axis=1) # drop id and target from X, since \"id\" wouldn't do much help for prediction\n",
    "Y = train['target'].as_matrix()\n",
    "X =X.fillna(X.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take a training set of size 200 000 (more than 30% of the set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-e713d67c593f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mListOfFunction\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mrfc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mrfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m200000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m200000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mY_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cyril2/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=np.float64,\n\u001b[0;32m-> 1173\u001b[0;31m                          order=\"C\")\n\u001b[0m\u001b[1;32m   1174\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cyril2/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    519\u001b[0m     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,\n\u001b[1;32m    520\u001b[0m                     \u001b[0mensure_2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m                     ensure_min_features, warn_on_dtype, estimator)\n\u001b[0m\u001b[1;32m    522\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m/home/cyril2/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    405\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[1;32m    406\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cyril2/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     56\u001b[0m             and not np.isfinite(X).all()):\n\u001b[1;32m     57\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[0;32m---> 58\u001b[0;31m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "for func in ListOfFunction:\n",
    "    rfc = func()\n",
    "    rfc.fit(X[:200000],Y[:200000])\n",
    "    Y_pred = rfc.predict_proba(X[-50000:])\n",
    "    print(func.__name__)\n",
    "    gini.gini_visualization(Y[-50000:],Y_pred,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly Gradient Boosting seems best by now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to remove some unuseful features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# When there is calc we remove (see first_examples notebook)\n",
    "customX = X.drop(X.filter(like='calc').columns,axis=1)\n",
    "\n",
    "rfc = GradientBoostingRegressor()\n",
    "rfc.fit(X[:200000],Y[:200000])\n",
    "Y_pred = rfc.predict(X[-50000:])\n",
    "print(func.__name__)\n",
    "gini.gini_visualization(Y[-50000:],Y_pred,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make change in the number of estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# When there is calc we remove (see first_examples notebook)\n",
    "customX = X.drop(X.filter(like='calc').columns,axis=1)\n",
    "\n",
    "numbers = [10,20,40,60,80,100,150,200,300,400,500]\n",
    "results = []\n",
    "\n",
    "for i in numbers:\n",
    "    rfc = GradientBoostingRegressor(n_estimators = i)\n",
    "    rfc.fit(X[:200000],Y[:200000])\n",
    "    Y_pred = rfc.predict(X[-50000:])\n",
    "    print(func.__name__ + \" for \"+str(i)+\" classifiers\")\n",
    "    results.append(gini.gini_visualization(Y[-50000:],Y_pred,False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Summary\")\n",
    "plt.plot(numbers,results)\n",
    "plt.xlabel('Number of classifiers')\n",
    "plt.ylabel('Gini')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More precised tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try with unuseful features and normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Case 1 : Feature removal + Normalization\n",
    "customX = X.drop(X.filter(like='calc').columns,axis=1)\n",
    "customX = normalize(customX)\n",
    "\n",
    "numbers = [60,80,100,150]\n",
    "results = []\n",
    "\n",
    "for i in numbers:\n",
    "    rfc = GradientBoostingRegressor(n_estimators = i)\n",
    "    rfc.fit(customX[:200000],Y[:200000])\n",
    "    Y_pred = rfc.predict(customX[-50000:])\n",
    "    print(func.__name__ + \" for \"+str(i)+\" classifiers\")\n",
    "    results.append(gini.gini_visualization(Y[-50000:],Y_pred,False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Case 2 : No feature removal + Normalization\n",
    "customX = normalize(X)\n",
    "\n",
    "numbers = [60,80,100,150] # these seems to be the best parameters\n",
    "results = []\n",
    "\n",
    "for i in numbers:\n",
    "    rfc = GradientBoostingRegressor(n_estimators = i)\n",
    "    rfc.fit(customX[:200000],Y[:200000])\n",
    "    Y_pred = rfc.predict(customX[-50000:])\n",
    "    print(func.__name__ + \" for \"+str(i)+\" classifiers\")\n",
    "    results.append(gini.gini_visualization(Y[-50000:],Y_pred,False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Case 3 : Feature removal + No normalization\n",
    "customX = X.drop(X.filter(like='calc').columns,axis=1)\n",
    "\n",
    "numbers = [60,80,100,150]\n",
    "results = []\n",
    "\n",
    "for i in numbers:\n",
    "    rfc = GradientBoostingRegressor(n_estimators = i)\n",
    "    rfc.fit(customX[:200000],Y[:200000])\n",
    "    Y_pred = rfc.predict(customX[-50000:])\n",
    "    print(func.__name__ + \" for \"+str(i)+\" classifiers\")\n",
    "    results.append(gini.gini_visualization(Y[-50000:],Y_pred,False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Case 4 : No feature removal, no normalization\n",
    "customX = X\n",
    "\n",
    "numbers = [60,80,100,150]\n",
    "results = []\n",
    "\n",
    "for i in numbers:\n",
    "    rfc = GradientBoostingRegressor(n_estimators = i)\n",
    "    rfc.fit(customX[:200000],Y[:200000])\n",
    "    Y_pred = rfc.predict(customX[-50000:])\n",
    "    print(func.__name__ + \" for \"+str(i)+\" classifiers\")\n",
    "    results.append(gini.gini_visualization(Y[-50000:],Y_pred,False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More features removal ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correlated_features = [\"ps_reg_03\",\"ps_car_13\"]\n",
    "lacunar_features = [\"ps_car_03_cat\",\"ps_car_05_cat\"]\n",
    "func = GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "customX = X.drop(X.filter(like='calc').columns,axis=1).drop(correlated_features,axis=1)\n",
    "\n",
    "numbers = [60,80,100,150,200]\n",
    "results = []\n",
    "\n",
    "for i in numbers:\n",
    "    rfc = GradientBoostingRegressor(n_estimators = i)\n",
    "    rfc.fit(customX[:200000],Y[:200000])\n",
    "    Y_pred = rfc.predict(customX[-50000:])\n",
    "    print(func.__name__ + \" for \"+str(i)+\" classifiers\")\n",
    "    results.append(gini.gini_visualization(Y[-50000:],Y_pred,False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "customX = X.drop(X.filter(like='calc').columns,axis=1).drop(lacunar_features,axis=1)\n",
    "\n",
    "numbers = [60,80,100,150,200]\n",
    "results = []\n",
    "\n",
    "for i in numbers:\n",
    "    rfc = GradientBoostingRegressor(n_estimators = i)\n",
    "    rfc.fit(customX[:200000],Y[:200000])\n",
    "    Y_pred = rfc.predict(customX[-50000:])\n",
    "    print(func.__name__ + \" for \"+str(i)+\" classifiers\")\n",
    "    results.append(gini.gini_visualization(Y[-50000:],Y_pred,False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "customX = X.drop(X.filter(like='calc').columns,axis=1).drop(correlated_features,axis=1).drop(lacunar_features,axis=1)\n",
    "\n",
    "numbers = [60,80,100,150,200]\n",
    "results = []\n",
    "\n",
    "for i in numbers:\n",
    "    rfc = GradientBoostingRegressor(n_estimators = i)\n",
    "    rfc.fit(customX[:200000],Y[:200000])\n",
    "    Y_pred = rfc.predict(customX[-50000:])\n",
    "    print(func.__name__ + \" for \"+str(i)+\" classifiers\")\n",
    "    results.append(gini.gini_visualization(Y[-50000:],Y_pred,False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More and More ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "customX = X.drop(X.filter(like='calc').columns,axis=1).drop(correlated_features,axis=1)\n",
    "disbalance = []\n",
    "\n",
    "for col in customX.filter(like='bin').columns:\n",
    "    if(train[[col,'id']].groupby([col], as_index=False).count().loc[1]['id']< 20000 or train[[col,'id']].groupby([col], as_index=False).count().loc[0]['id']< 20000):\n",
    "        print(col)\n",
    "        disbalance.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "customX = X.drop(X.filter(like='calc').columns,axis=1).drop(correlated_features,axis=1).drop(lacunar_features,axis=1).drop(disbalance,axis=1)\n",
    "\n",
    "numbers = [60,80,100,150,200]\n",
    "results = []\n",
    "\n",
    "for i in numbers:\n",
    "    rfc = GradientBoostingRegressor(n_estimators = i)\n",
    "    rfc.fit(customX[:200000],Y[:200000])\n",
    "    Y_pred = rfc.predict(customX[-50000:])\n",
    "    print(func.__name__ + \" for \"+str(i)+\" classifiers\")\n",
    "    results.append(gini.gini_visualization(Y[-50000:],Y_pred,False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "customX = X.drop(X.filter(like='calc').columns,axis=1).drop(correlated_features,axis=1).drop(lacunar_features,axis=1).drop(disbalance,axis=1)\n",
    "disbalance2 = []\n",
    "\n",
    "for col in X.filter(like='cat').columns:\n",
    "    if(max(train[[col,'id']].groupby([col], as_index=False).count().id)>510000):\n",
    "        print(col)\n",
    "        disbalance2.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "customX = X.drop(X.filter(like='calc').columns,axis=1).drop(correlated_features,axis=1).drop(lacunar_features,axis=1).drop(disbalance,axis=1)\n",
    "\n",
    "numbers = [150]\n",
    "results = []\n",
    "\n",
    "for i in numbers:\n",
    "    rfc = GradientBoostingRegressor(n_estimators = i)\n",
    "    rfc.fit(customX,Y)\n",
    "    Y_pred = rfc.predict(customX)\n",
    "    print(func.__name__ + \" for \"+str(i)+\" classifiers\")\n",
    "    results.append(gini.gini_visualization(Y,Y_pred,False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = []\n",
    "testX = test.drop([\"id\"],axis=1).drop(X.filter(like='calc').columns,axis=1).drop(correlated_features,axis=1).drop(lacunar_features,axis=1).drop(disbalance,axis=1)\n",
    "Y_pred = rfc.predict(testX)\n",
    "\n",
    "k=0\n",
    "for i,p in zip(test.id, Y_pred):\n",
    "    res.append([i,max(0,p)])\n",
    "    \n",
    "pd.DataFrame(res,columns=[\"id\",\"target\"]).to_csv(\"prediction.csv\",index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RFECV(cv=None,\n",
       "   estimator=GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=150,\n",
       "             presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "             warm_start=False),\n",
       "   n_jobs=1, scoring=None, step=1, verbose=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = GradientBoostingRegressor(n_estimators = 150)\n",
    "selector = RFECV(rfc,step = 1)\n",
    "selector.fit(X, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  1,  1,  1,  1,  1,  1,  1,  1,  3,  5,  1,  1,  1,  1,  1,  1,\n",
       "       16,  1,  1,  1,  1,  6,  1,  1, 12,  1,  1,  1,  1, 13,  1,  1,  1,\n",
       "        1,  1,  1,  8,  1,  9,  4,  1,  7,  1,  1, 10,  1,  1,  2,  1,  1,\n",
       "       14, 15, 18, 17, 11, 19])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector.ranking_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
